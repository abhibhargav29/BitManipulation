{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Shape of train data: (3317, 3)\n",
      "Final shape of train dataset: (2840, 3)\n",
      "                                                text  intensity  emotion\n",
      "0  How the fu*k! Who the heck! moved my fridge!.....      0.938        0\n",
      "1  So my Indian Uber driver just called someone t...      0.896        0\n",
      "2  @DPD_UK I asked for my parcel to be delivered ...      0.896        0\n",
      "3  so ef whichever butt wipe pulled the fire alar...      0.896        0\n",
      "4  Don't join @BTCare they put the phone down on ...      0.896        0\n",
      "5                                My blood is boiling      0.875        0\n",
      "6  When you've still got a whole season of Wentwo...      0.875        0\n",
      "7  @bt_uk why does tracking show my equipment del...      0.875        0\n",
      "8  @TeamShanny legit why i am so furious with him...      0.875        0\n",
      "9  How is it suppose to work if you do that? Wtf ...      0.875        0\n",
      "\n",
      "Initial Shape of CV data: (347, 3)\n",
      "Final shape of CV data: (296, 3)\n",
      "                                                text  intensity  emotion\n",
      "0  @ZubairSabirPTI  pls dont insult the word 'Molna'      0.479        0\n",
      "1  @ArcticFantasy I would have almost took offens...      0.458        0\n",
      "2  @IllinoisLoyalty that Rutgers game was an abom...      0.562        0\n",
      "3  @CozanGaming that's what lisa asked before she...      0.500        0\n",
      "4  Sometimes I get mad over something so minuscul...      0.708        0\n",
      "5  Sometimes I get mad over something so minuscul...      0.646        0\n",
      "6  My eyes have been dilated. I hate the world ri...      0.812        0\n",
      "7  @huwellwell One chosen by the CLP members! MP ...      0.682        0\n",
      "8  @huwellwell One chosen by the CLP members! MP ...      0.438        0\n",
      "9  @Yoshi_OnoChin can you please not have Canadia...      0.646        0\n",
      "\n",
      "Initial Shape of Test data: (1454, 3)\n",
      "Final shape of Test data: (998, 3)\n",
      "                                                text  intensity  emotion\n",
      "0  This game has pissed me off more than any othe...      0.898        0\n",
      "1  @moocowward @mrsajhargreaves @Melly77 @GaryBar...      0.646        0\n",
      "2  @moocowward @mrsajhargreaves @Melly77 @GaryBar...      0.583        0\n",
      "3  @virginmedia I've been disconnected whilst on ...      0.625        0\n",
      "4       @shae_caitlin ur road rage gives me anxiety.      0.438        0\n",
      "5  @eMilsOnWheels I'm furious Ã°Å¸ËœÂ©Ã°Å¸ËœÂ©Ã°Å...      0.708        0\n",
      "6  They gonna give this KKk police bitch the mini...      0.877        0\n",
      "7  They gonna give this KKk police bitch the mini...      0.708        0\n",
      "8           I just got murdered in madden. Ã°Å¸Â¤â€¢      0.417        0\n",
      "9  new madden 16 video was gonna be up but xbox i...      0.667        0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Train Data\n",
    "trainDf =  pd.read_csv('Train.txt', sep='\t', names=[\"id\",\"text\",\"emotion\",\"intensity\"], engine='python')\n",
    "trainDf=trainDf.drop(\"id\",axis=1)\n",
    "print(\"Initial Shape of train data:\",trainDf.shape)\n",
    "\n",
    "AngerDf = trainDf[trainDf[\"emotion\"]==\"anger\"].drop(\"emotion\",axis=1)\n",
    "AngerDf = AngerDf[AngerDf[\"intensity\"]>=0.35]\n",
    "AngerDf[\"emotion\"] = 0\n",
    "\n",
    "FearDf = trainDf[trainDf[\"emotion\"]==\"fear\"].drop(\"emotion\",axis=1)\n",
    "FearDf = FearDf[FearDf[\"intensity\"]>=0.4]\n",
    "FearDf[\"emotion\"] = 1\n",
    "\n",
    "JoyDf = trainDf[trainDf[\"emotion\"]==\"joy\"].drop(\"emotion\",axis=1)\n",
    "JoyDf = JoyDf[JoyDf[\"intensity\"]>=0.292]\n",
    "JoyDf[\"emotion\"] = 2\n",
    "\n",
    "SadDf = trainDf[trainDf[\"emotion\"]==\"sadness\"].drop(\"emotion\",axis=1)\n",
    "SadDf = SadDf[SadDf[\"intensity\"]>=0.263]\n",
    "SadDf[\"emotion\"] = 3\n",
    "\n",
    "trainDf = pd.concat([AngerDf,FearDf,JoyDf,SadDf],ignore_index=True)\n",
    "print(\"Final shape of train dataset:\",trainDf.shape)\n",
    "print(trainDf.head(10))\n",
    "print()\n",
    "\n",
    "\n",
    "#Cross Validation Data\n",
    "crossValDf =  pd.read_csv('CrossValidate.txt', sep='\t', names=[\"id\",\"text\",\"emotion\",\"intensity\"], engine='python')\n",
    "crossValDf=crossValDf.drop(\"id\",axis=1)\n",
    "print(\"Initial Shape of CV data:\",crossValDf.shape)\n",
    "\n",
    "\n",
    "AngerDf = crossValDf[crossValDf[\"emotion\"]==\"anger\"].drop(\"emotion\",axis=1)\n",
    "AngerDf = AngerDf[AngerDf[\"intensity\"]>0.271]\n",
    "AngerDf[\"emotion\"] = 0\n",
    "\n",
    "FearDf = crossValDf[crossValDf[\"emotion\"]==\"fear\"].drop(\"emotion\",axis=1)\n",
    "FearDf = FearDf[FearDf[\"intensity\"]>=0.250]\n",
    "FearDf[\"emotion\"] = 1\n",
    "\n",
    "JoyDf = crossValDf[crossValDf[\"emotion\"]==\"joy\"].drop(\"emotion\",axis=1)\n",
    "JoyDf = JoyDf[JoyDf[\"intensity\"]>=0.292]\n",
    "JoyDf[\"emotion\"] = 2\n",
    "\n",
    "SadDf = crossValDf[crossValDf[\"emotion\"]==\"sadness\"].drop(\"emotion\",axis=1)\n",
    "SadDf = SadDf[SadDf[\"intensity\"]>=0.333]\n",
    "SadDf[\"emotion\"] = 3\n",
    "\n",
    "crossValDf = pd.concat([AngerDf,FearDf,JoyDf,SadDf],ignore_index=True)\n",
    "print(\"Final shape of CV data:\",crossValDf.shape)\n",
    "print(crossValDf.head(10))\n",
    "print()\n",
    "\n",
    "\n",
    "#Test Data\n",
    "testDf =  pd.read_csv('Test.txt', sep='\t', names=[\"id\",\"text\",\"emotion\",\"intensity\"], engine='python')\n",
    "testDf=testDf.drop(\"id\",axis=1)\n",
    "print(\"Initial Shape of Test data:\",testDf.shape)\n",
    "\n",
    "\n",
    "AngerDf = testDf[testDf[\"emotion\"]==\"anger\"].drop(\"emotion\",axis=1)\n",
    "AngerDf = AngerDf[AngerDf[\"intensity\"]>=0.400]\n",
    "AngerDf[\"emotion\"] = 0\n",
    "\n",
    "FearDf = testDf[testDf[\"emotion\"]==\"fear\"].drop(\"emotion\",axis=1)\n",
    "FearDf = FearDf[FearDf[\"intensity\"]>=0.438]\n",
    "FearDf[\"emotion\"] = 1\n",
    "\n",
    "JoyDf = testDf[testDf[\"emotion\"]==\"joy\"].drop(\"emotion\",axis=1)\n",
    "JoyDf = JoyDf[JoyDf[\"intensity\"]>=0.400]\n",
    "JoyDf[\"emotion\"] = 2\n",
    "\n",
    "SadDf = testDf[testDf[\"emotion\"]==\"sadness\"].drop(\"emotion\",axis=1)\n",
    "SadDf = SadDf[SadDf[\"intensity\"]>=0.354]\n",
    "SadDf[\"emotion\"] = 3\n",
    "\n",
    "testDf = pd.concat([AngerDf,FearDf,JoyDf,SadDf],ignore_index=True)\n",
    "print(\"Final shape of Test data:\",testDf.shape)\n",
    "print(testDf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train= trainDf.drop([\"intensity\"],axis=1)\n",
    "data_cv   = crossValDf.drop([\"intensity\"],axis=1)\n",
    "data_test = testDf.drop([\"intensity\"],axis=1)\n",
    "\n",
    "# emotion=0 means anger\n",
    "# emotion=1 means fear\n",
    "# emotion=2 means joy\n",
    "# emotion=3 means sadness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  emotion\n",
      "0  [fuk, heck, move, fridg, knock, landlord, door...        0\n",
      "1  [indian, uber, driver, call, someon, n, word, ...        0\n",
      "2  [ask, parcel, deliv, pick, store, address, fum...        0\n",
      "3  [ef, whichev, butt, wipe, pull, fire, alarm, d...        0\n",
      "4  [join, btcare, put, phone, talk, rude, take, m...        0\n",
      "5                                      [blood, boil]        0\n",
      "6  [still, got, whole, season, wentworth, watch, ...        0\n",
      "7  [track, show, equip, deliv, servic, sudden, de...        0\n",
      "8               [legit, furious, peopl, fuck, idiot]        0\n",
      "9             [suppos, work, wtf, dude, thank, piss]        0\n",
      "\n",
      "                                                text  emotion\n",
      "0                   [pls, dont, insult, word, molna]        0\n",
      "1        [would, almost, took, offens, actual, snap]        0\n",
      "2  [rutger, game, abomin, affront, god, man, must...        0\n",
      "3                [lisa, ask, start, rage, call, heh]        0\n",
      "4  [sometim, get, mad, someth, minuscul, tri, rui...        0\n",
      "5  [sometim, get, mad, someth, minuscul, tri, rui...        0\n",
      "6  [eye, dilat, hate, world, right, rage, thousan...        0\n",
      "7  [one, chosen, clp, member, mp, seat, peopl, do...        0\n",
      "8  [one, chosen, clp, member, mp, seat, peopl, do...        0\n",
      "9  [pleas, canadian, player, play, us, player, la...        0\n",
      "\n",
      "                                                text  emotion\n",
      "0  [game, piss, game, year, blood, boil, time, tu...        0\n",
      "1  [mrsajhargreav, melli, garybarlow, come, mum, ...        0\n",
      "2  [mrsajhargreav, melli, garybarlow, come, mum, ...        0\n",
      "3  [disconnect, whilst, holiday, move, hous, st, ...        0\n",
      "4                    [ur, road, rage, give, anxieti]        0\n",
      "5                                          [furious]        0\n",
      "6  [gonna, give, kkk, polic, bitch, minimum, sent...        0\n",
      "7  [gonna, give, kkk, polic, bitch, minimum, sent...        0\n",
      "8                              [got, murder, madden]        0\n",
      "9  [new, madden, video, gonna, xbox, ahol, go, st...        0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "#Get set of english stop words and prepare stemmer\n",
    "stop=set(stopwords.words(\"english\"))\n",
    "sno = nltk.stem.SnowballStemmer(\"english\")\n",
    "\n",
    "#Train data cleaning\n",
    "train_text=data_train[\"text\"]\n",
    "cleaned_text=[]\n",
    "for line in train_text:\n",
    "    #Removing tags(ex-@abhishek is a name and not needed)\n",
    "    tags = re.compile(\"^@[a-zA-Z_]*\")\n",
    "    line = re.sub(tags,\" \",line)\n",
    "    #Replacing # and * with a \"\"\n",
    "    hashtags = re.compile(\"#|\\*\")\n",
    "    line = re.sub(hashtags,\"\",line)\n",
    "    #Replacing all other characters with a space\n",
    "    extraCharacters = re.compile(\"[^a-zA-Z]\")\n",
    "    line = re.sub(extraCharacters,\" \",line)\n",
    "    filtered_words=[]\n",
    "    \n",
    "    #Conversion of line to array of words\n",
    "    for word in line.split():\n",
    "        word=word.lower()\n",
    "        if(word not in stop):\n",
    "            word = sno.stem(word)\n",
    "            filtered_words.append(word)\n",
    "    cleaned_text.append(filtered_words)\n",
    "            \n",
    "data_train[\"text\"]=cleaned_text\n",
    "print(data_train.head(10))\n",
    "print()\n",
    "\n",
    "#Cross Validate data cleaning\n",
    "cv_text=data_cv[\"text\"][0:]\n",
    "cleaned_text=[]\n",
    "for line in cv_text:\n",
    "    #Removing tags(ex-@abhishek is a name and not needed)\n",
    "    tags = re.compile(\"^@[a-zA-Z_]*\")\n",
    "    line = re.sub(tags,\" \",line)\n",
    "    #Replacing # and * with \"\" \n",
    "    hashtags = re.compile(\"#|\\*\")\n",
    "    line = re.sub(hashtags,\"\",line)\n",
    "    #Replacing all other characters with a space\n",
    "    extraCharacters = re.compile(\"[^a-zA-Z]\")\n",
    "    line = re.sub(extraCharacters,\" \",line)\n",
    "    \n",
    "    #Conversion of line to array of words\n",
    "    filtered_words=[]\n",
    "    for word in line.split():\n",
    "        word=word.lower()\n",
    "        if(word not in stop):\n",
    "            word = sno.stem(word)\n",
    "            filtered_words.append(word)\n",
    "    cleaned_text.append(filtered_words)\n",
    "            \n",
    "data_cv[\"text\"]=cleaned_text\n",
    "print(data_cv.head(10))\n",
    "print()\n",
    "\n",
    "#Test data cleaning\n",
    "test_text=data_test[\"text\"][0:]\n",
    "cleaned_text=[]\n",
    "for line in test_text:\n",
    "    #Removing tags(ex-@abhishek is a name and not needed)\n",
    "    tags = re.compile(\"^@[a-zA-Z_]*\")\n",
    "    line = re.sub(tags,\" \",line)\n",
    "    #Replacing hash and * with \"\"\n",
    "    hashtags = re.compile(\"#|\\*\")\n",
    "    line = re.sub(hashtags,\"\",line)\n",
    "    #Replacing all other characters with a space\n",
    "    extraCharacters = re.compile(\"[^a-zA-Z]\")\n",
    "    line = re.sub(extraCharacters,\" \",line)\n",
    "    \n",
    "    #Conversion of line to array of words\n",
    "    filtered_words=[]\n",
    "    for word in line.split():\n",
    "        word=word.lower()\n",
    "        if(word not in stop):\n",
    "            word = sno.stem(word)\n",
    "            filtered_words.append(word)\n",
    "    cleaned_text.append(filtered_words)\n",
    "            \n",
    "data_test[\"text\"]=cleaned_text\n",
    "print(data_test.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2840/2840 [00:00<00:00, 5338.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.010811 -0.002395 -0.003304 -0.013111  0.004599 -0.010065 -0.012781   \n",
      "1  0.016901 -0.003309 -0.006856 -0.022324  0.007567 -0.017228 -0.021352   \n",
      "2  0.008887 -0.002072 -0.003029 -0.011886  0.004473 -0.009074 -0.011522   \n",
      "3  0.009685 -0.002460 -0.003401 -0.012250  0.004529 -0.009266 -0.011931   \n",
      "4  0.014243 -0.003578 -0.005921 -0.018841  0.006535 -0.014779 -0.017013   \n",
      "5  0.008095 -0.002186 -0.003142 -0.011746  0.004185 -0.009562 -0.009984   \n",
      "6  0.019486 -0.005017 -0.007621 -0.026569  0.009730 -0.020312 -0.024442   \n",
      "7  0.011808 -0.002903 -0.003815 -0.016110  0.006302 -0.012099 -0.014492   \n",
      "8  0.020335 -0.005600 -0.008251 -0.028469  0.009450 -0.022262 -0.025520   \n",
      "9  0.011424 -0.003358 -0.003669 -0.014964  0.005508 -0.011640 -0.013327   \n",
      "\n",
      "          7         8         9  ...       291       292       293       294  \\\n",
      "0 -0.004045  0.006342 -0.007068  ... -0.008057 -0.001864 -0.002956 -0.017881   \n",
      "1 -0.006573  0.010405 -0.010853  ... -0.014322 -0.003348 -0.004232 -0.029529   \n",
      "2 -0.003451  0.006083 -0.005298  ... -0.007520 -0.001909 -0.002295 -0.016595   \n",
      "3 -0.003674  0.005850 -0.005523  ... -0.007497 -0.001781 -0.002196 -0.016580   \n",
      "4 -0.005671  0.008832 -0.009061  ... -0.011988 -0.003218 -0.003585 -0.024965   \n",
      "5 -0.003992  0.006249 -0.006170  ... -0.007243 -0.001428 -0.001059 -0.014569   \n",
      "6 -0.008140  0.011656 -0.012775  ... -0.016185 -0.003680 -0.005285 -0.035185   \n",
      "7 -0.004934  0.007991 -0.007334  ... -0.009460 -0.001835 -0.003722 -0.021555   \n",
      "8 -0.008390  0.013079 -0.012623  ... -0.015974 -0.003852 -0.005732 -0.038139   \n",
      "9 -0.004552  0.006659 -0.006917  ... -0.009419 -0.001802 -0.002650 -0.018701   \n",
      "\n",
      "        295       296       297       298       299  emotion  \n",
      "0  0.003728  0.012489  0.014361 -0.002623  0.000624        0  \n",
      "1  0.005646  0.020475  0.023437 -0.004569 -0.000221        0  \n",
      "2  0.002868  0.010123  0.013054 -0.002138 -0.000601        0  \n",
      "3  0.003238  0.011701  0.013293 -0.002042 -0.000003        0  \n",
      "4  0.004534  0.016905  0.020088 -0.004224 -0.000100        0  \n",
      "5  0.002944  0.011000  0.011344 -0.002921 -0.000587        0  \n",
      "6  0.007411  0.023388  0.028106 -0.005430 -0.000170        0  \n",
      "7  0.003501  0.013947  0.016922 -0.002983 -0.000629        0  \n",
      "8  0.007227  0.023995  0.028684 -0.006559 -0.000479        0  \n",
      "9  0.003389  0.013439  0.015112 -0.002765 -0.000952        0  \n",
      "\n",
      "[10 rows x 301 columns]\n",
      "(2840, 301)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Train on text data\n",
    "list_of_sent = data_train[\"text\"]\n",
    "w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=300, workers=4)\n",
    "w2v_words=list(w2v_model.wv.vocab)\n",
    "\n",
    "#Vectorize text data\n",
    "listof_sent_vec=[]\n",
    "#tqdm is for improving speed and making progressbar\n",
    "#Vectorization and normalization both going on\n",
    "for sent in tqdm(list_of_sent): \n",
    "    sent_vec = np.zeros(300) \n",
    "    cnt_words =0; \n",
    "    for word in sent: \n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    listof_sent_vec.append(sent_vec)\n",
    "    \n",
    "Label = data_train[\"emotion\"]\n",
    "list_col=tuple(range(300))\n",
    "W2v_data_train=pd.DataFrame(data=listof_sent_vec, columns=list_col)\n",
    "W2v_data_train[\"emotion\"] = Label\n",
    "print(W2v_data_train.head(10))\n",
    "print(W2v_data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 296/296 [00:00<00:00, 6165.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.008337 -0.000875 -0.003654 -0.012183  0.003616 -0.009058 -0.011530   \n",
      "1  0.012515 -0.002597 -0.005005 -0.015760  0.005453 -0.012741 -0.015162   \n",
      "2  0.016371 -0.003254 -0.005998 -0.022280  0.007538 -0.016211 -0.019662   \n",
      "3  0.021432 -0.004745 -0.008489 -0.027819  0.009653 -0.021888 -0.025948   \n",
      "4  0.022636 -0.005382 -0.008728 -0.030269  0.010977 -0.022794 -0.028101   \n",
      "5  0.022878 -0.005374 -0.008810 -0.030510  0.010978 -0.023132 -0.028477   \n",
      "6  0.016644 -0.004017 -0.006443 -0.022135  0.007325 -0.017237 -0.020397   \n",
      "7  0.013912 -0.003019 -0.004869 -0.018542  0.006737 -0.014772 -0.017602   \n",
      "8  0.014304 -0.003282 -0.005020 -0.018672  0.006891 -0.015207 -0.018162   \n",
      "9  0.016041 -0.003655 -0.005263 -0.020422  0.006930 -0.016967 -0.020403   \n",
      "\n",
      "          7         8         9  ...       291       292       293       294  \\\n",
      "0 -0.004037  0.005434 -0.005804  ... -0.008315 -0.001646 -0.001818 -0.016432   \n",
      "1 -0.004520  0.007978 -0.007395  ... -0.011111 -0.002436 -0.002659 -0.020890   \n",
      "2 -0.006160  0.009971 -0.009968  ... -0.013439 -0.003383 -0.004407 -0.028677   \n",
      "3 -0.007872  0.013317 -0.013112  ... -0.017389 -0.004351 -0.005821 -0.036639   \n",
      "4 -0.008973  0.013756 -0.014536  ... -0.018725 -0.004152 -0.005821 -0.040084   \n",
      "5 -0.009137  0.013789 -0.014645  ... -0.018971 -0.004159 -0.005873 -0.040339   \n",
      "6 -0.006513  0.010305 -0.010649  ... -0.013803 -0.002765 -0.004813 -0.029650   \n",
      "7 -0.005329  0.008624 -0.009132  ... -0.011940 -0.002929 -0.003500 -0.025916   \n",
      "8 -0.005496  0.008600 -0.009511  ... -0.012162 -0.002805 -0.003531 -0.026331   \n",
      "9 -0.006392  0.009109 -0.010507  ... -0.012333 -0.003309 -0.003879 -0.027529   \n",
      "\n",
      "        295       296       297       298       299  emotion  \n",
      "0  0.002668  0.010638  0.011788 -0.002560 -0.000050        0  \n",
      "1  0.004965  0.013530  0.016962 -0.003383 -0.000050        0  \n",
      "2  0.005516  0.019376  0.023388 -0.005088 -0.000108        0  \n",
      "3  0.007345  0.024705  0.029575 -0.006333  0.000083        0  \n",
      "4  0.007508  0.027054  0.031455 -0.006349 -0.000138        0  \n",
      "5  0.007654  0.027229  0.031673 -0.006373 -0.000132        0  \n",
      "6  0.006003  0.019146  0.023509 -0.004389 -0.000219        0  \n",
      "7  0.004657  0.016446  0.020027 -0.003959 -0.000926        0  \n",
      "8  0.004841  0.016919  0.020292 -0.004187 -0.000890        0  \n",
      "9  0.005623  0.018678  0.022128 -0.004565  0.000270        0  \n",
      "\n",
      "[10 rows x 301 columns]\n",
      "(296, 301)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 998/998 [00:00<00:00, 4352.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.017389 -0.003800 -0.006450 -0.023239  0.007974 -0.017850 -0.021276   \n",
      "1  0.010864 -0.002100 -0.003431 -0.014579  0.006130 -0.011176 -0.013766   \n",
      "2  0.011196 -0.002033 -0.003909 -0.015354  0.006477 -0.011564 -0.014669   \n",
      "3  0.008719 -0.002118 -0.003537 -0.012290  0.003580 -0.009142 -0.011427   \n",
      "4  0.017617 -0.004084 -0.007694 -0.023714  0.008318 -0.017596 -0.022531   \n",
      "5  0.005844 -0.002830 -0.001263 -0.008921  0.003466 -0.007650 -0.008355   \n",
      "6  0.015716 -0.003962 -0.006178 -0.021699  0.008709 -0.015502 -0.020882   \n",
      "7  0.014414 -0.003580 -0.006042 -0.020682  0.008283 -0.014645 -0.019650   \n",
      "8  0.015586 -0.002803 -0.006083 -0.021088  0.007486 -0.015192 -0.017898   \n",
      "9  0.015006 -0.003439 -0.005608 -0.021378  0.007912 -0.016516 -0.019343   \n",
      "\n",
      "          7         8         9  ...       291       292       293       294  \\\n",
      "0 -0.006924  0.010780 -0.010771  ... -0.014549 -0.003471 -0.004094 -0.030183   \n",
      "1 -0.004604  0.006951 -0.007099  ... -0.008629 -0.001608 -0.003058 -0.019837   \n",
      "2 -0.004545  0.007503 -0.007410  ... -0.009099 -0.001536 -0.003109 -0.021064   \n",
      "3 -0.003268  0.005301 -0.005436  ... -0.006951 -0.001708 -0.002839 -0.015011   \n",
      "4 -0.006543  0.011260 -0.011192  ... -0.014086 -0.002981 -0.005343 -0.031457   \n",
      "5 -0.001781  0.004005 -0.002488  ... -0.003528  0.000006 -0.001831 -0.011588   \n",
      "6 -0.005324  0.009579 -0.009513  ... -0.011730 -0.003166 -0.004472 -0.028375   \n",
      "7 -0.005105  0.008771 -0.008590  ... -0.010994 -0.002884 -0.004195 -0.027081   \n",
      "8 -0.005119  0.008676 -0.009736  ... -0.011974 -0.002586 -0.005209 -0.026234   \n",
      "9 -0.005477  0.008521 -0.009629  ... -0.012267 -0.004173 -0.004760 -0.027396   \n",
      "\n",
      "        295       296       297       298       299  emotion  \n",
      "0  0.006005  0.021005  0.023736 -0.005608  0.000463        0  \n",
      "1  0.003382  0.012739  0.015344 -0.002938  0.000629        0  \n",
      "2  0.003868  0.013188  0.016256 -0.003390  0.000587        0  \n",
      "3  0.003556  0.009846  0.012222 -0.001985 -0.000490        0  \n",
      "4  0.006778  0.020774  0.024172 -0.005068 -0.000379        0  \n",
      "5  0.003095  0.006121  0.007182 -0.001413 -0.001225        0  \n",
      "6  0.005509  0.017952  0.021833 -0.004125 -0.000281        0  \n",
      "7  0.005416  0.016862  0.020883 -0.004072 -0.000479        0  \n",
      "8  0.005978  0.017600  0.020200 -0.004056 -0.000726        0  \n",
      "9  0.005887  0.018439  0.021178 -0.004016 -0.000476        0  \n",
      "\n",
      "[10 rows x 301 columns]\n",
      "(998, 301)\n"
     ]
    }
   ],
   "source": [
    "#Vectorize Cross Validate\n",
    "list_of_sent= data_cv[\"text\"]\n",
    "listof_sent_vec=[]\n",
    "\n",
    "for sent in tqdm(list_of_sent): \n",
    "    sent_vec = np.zeros(300) \n",
    "    cnt_words =0; \n",
    "    for word in sent: \n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    listof_sent_vec.append(sent_vec)\n",
    "    \n",
    "Label = data_cv[\"emotion\"]\n",
    "list_col=tuple(range(300))\n",
    "W2v_data_cv=pd.DataFrame(data=listof_sent_vec, columns=list_col)\n",
    "W2v_data_cv[\"emotion\"] = Label\n",
    "print(W2v_data_cv.head(10))\n",
    "print(W2v_data_cv.shape)\n",
    "\n",
    "\n",
    "#test\n",
    "list_of_sent= data_test[\"text\"]\n",
    "listof_sent_vec=[]\n",
    "\n",
    "for sent in tqdm(list_of_sent): \n",
    "    sent_vec = np.zeros(300) \n",
    "    cnt_words =0; \n",
    "    for word in sent: \n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    listof_sent_vec.append(sent_vec)\n",
    "    \n",
    "Label = data_test[\"emotion\"]\n",
    "list_col=tuple(range(300))\n",
    "W2v_data_test=pd.DataFrame(data=listof_sent_vec, columns=list_col)\n",
    "W2v_data_test[\"emotion\"] = Label\n",
    "print(W2v_data_test.head(10))\n",
    "print(W2v_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
