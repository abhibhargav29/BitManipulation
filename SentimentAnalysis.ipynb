{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Shape of train data: (3317, 3)\n",
      "Final shape of train dataset: (2840, 3)\n",
      "                                                text  intensity  emotion\n",
      "0  How the fu*k! Who the heck! moved my fridge!.....      0.938        0\n",
      "1  So my Indian Uber driver just called someone t...      0.896        0\n",
      "2  @DPD_UK I asked for my parcel to be delivered ...      0.896        0\n",
      "3  so ef whichever butt wipe pulled the fire alar...      0.896        0\n",
      "4  Don't join @BTCare they put the phone down on ...      0.896        0\n",
      "5                                My blood is boiling      0.875        0\n",
      "6  When you've still got a whole season of Wentwo...      0.875        0\n",
      "7  @bt_uk why does tracking show my equipment del...      0.875        0\n",
      "8  @TeamShanny legit why i am so furious with him...      0.875        0\n",
      "9  How is it suppose to work if you do that? Wtf ...      0.875        0\n",
      "\n",
      "Initial Shape of CV data: (347, 3)\n",
      "Final shape of CV data: (296, 3)\n",
      "                                                text  intensity  emotion\n",
      "0  @ZubairSabirPTI  pls dont insult the word 'Molna'      0.479        0\n",
      "1  @ArcticFantasy I would have almost took offens...      0.458        0\n",
      "2  @IllinoisLoyalty that Rutgers game was an abom...      0.562        0\n",
      "3  @CozanGaming that's what lisa asked before she...      0.500        0\n",
      "4  Sometimes I get mad over something so minuscul...      0.708        0\n",
      "5  Sometimes I get mad over something so minuscul...      0.646        0\n",
      "6  My eyes have been dilated. I hate the world ri...      0.812        0\n",
      "7  @huwellwell One chosen by the CLP members! MP ...      0.682        0\n",
      "8  @huwellwell One chosen by the CLP members! MP ...      0.438        0\n",
      "9  @Yoshi_OnoChin can you please not have Canadia...      0.646        0\n",
      "\n",
      "Initial Shape of Test data: (1454, 3)\n",
      "Final shape of Test data: (998, 3)\n",
      "                                                text  intensity  emotion\n",
      "0  This game has pissed me off more than any othe...      0.898        0\n",
      "1  @moocowward @mrsajhargreaves @Melly77 @GaryBar...      0.646        0\n",
      "2  @moocowward @mrsajhargreaves @Melly77 @GaryBar...      0.583        0\n",
      "3  @virginmedia I've been disconnected whilst on ...      0.625        0\n",
      "4       @shae_caitlin ur road rage gives me anxiety.      0.438        0\n",
      "5  @eMilsOnWheels I'm furious Ã°Å¸ËœÂ©Ã°Å¸ËœÂ©Ã°Å...      0.708        0\n",
      "6  They gonna give this KKk police bitch the mini...      0.877        0\n",
      "7  They gonna give this KKk police bitch the mini...      0.708        0\n",
      "8           I just got murdered in madden. Ã°Å¸Â¤â€¢      0.417        0\n",
      "9  new madden 16 video was gonna be up but xbox i...      0.667        0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Train Data\n",
    "trainDf =  pd.read_csv('Train.txt', sep='\t', names=[\"id\",\"text\",\"emotion\",\"intensity\"], engine='python')\n",
    "trainDf=trainDf.drop(\"id\",axis=1)\n",
    "print(\"Initial Shape of train data:\",trainDf.shape)\n",
    "\n",
    "AngerDf = trainDf[trainDf[\"emotion\"]==\"anger\"].drop(\"emotion\",axis=1)\n",
    "AngerDf = AngerDf[AngerDf[\"intensity\"]>=0.35]\n",
    "AngerDf[\"emotion\"] = 0\n",
    "\n",
    "FearDf = trainDf[trainDf[\"emotion\"]==\"fear\"].drop(\"emotion\",axis=1)\n",
    "FearDf = FearDf[FearDf[\"intensity\"]>=0.4]\n",
    "FearDf[\"emotion\"] = 1\n",
    "\n",
    "JoyDf = trainDf[trainDf[\"emotion\"]==\"joy\"].drop(\"emotion\",axis=1)\n",
    "JoyDf = JoyDf[JoyDf[\"intensity\"]>=0.292]\n",
    "JoyDf[\"emotion\"] = 2\n",
    "\n",
    "SadDf = trainDf[trainDf[\"emotion\"]==\"sadness\"].drop(\"emotion\",axis=1)\n",
    "SadDf = SadDf[SadDf[\"intensity\"]>=0.263]\n",
    "SadDf[\"emotion\"] = 3\n",
    "\n",
    "trainDf = pd.concat([AngerDf,FearDf,JoyDf,SadDf],ignore_index=True)\n",
    "print(\"Final shape of train dataset:\",trainDf.shape)\n",
    "print(trainDf.head(10))\n",
    "print()\n",
    "\n",
    "\n",
    "#Cross Validation Data\n",
    "crossValDf =  pd.read_csv('CrossValidate.txt', sep='\t', names=[\"id\",\"text\",\"emotion\",\"intensity\"], engine='python')\n",
    "crossValDf=crossValDf.drop(\"id\",axis=1)\n",
    "print(\"Initial Shape of CV data:\",crossValDf.shape)\n",
    "\n",
    "\n",
    "AngerDf = crossValDf[crossValDf[\"emotion\"]==\"anger\"].drop(\"emotion\",axis=1)\n",
    "AngerDf = AngerDf[AngerDf[\"intensity\"]>0.271]\n",
    "AngerDf[\"emotion\"] = 0\n",
    "\n",
    "FearDf = crossValDf[crossValDf[\"emotion\"]==\"fear\"].drop(\"emotion\",axis=1)\n",
    "FearDf = FearDf[FearDf[\"intensity\"]>=0.250]\n",
    "FearDf[\"emotion\"] = 1\n",
    "\n",
    "JoyDf = crossValDf[crossValDf[\"emotion\"]==\"joy\"].drop(\"emotion\",axis=1)\n",
    "JoyDf = JoyDf[JoyDf[\"intensity\"]>=0.292]\n",
    "JoyDf[\"emotion\"] = 2\n",
    "\n",
    "SadDf = crossValDf[crossValDf[\"emotion\"]==\"sadness\"].drop(\"emotion\",axis=1)\n",
    "SadDf = SadDf[SadDf[\"intensity\"]>=0.333]\n",
    "SadDf[\"emotion\"] = 3\n",
    "\n",
    "crossValDf = pd.concat([AngerDf,FearDf,JoyDf,SadDf],ignore_index=True)\n",
    "print(\"Final shape of CV data:\",crossValDf.shape)\n",
    "print(crossValDf.head(10))\n",
    "print()\n",
    "\n",
    "\n",
    "#Test Data\n",
    "testDf =  pd.read_csv('Test.txt', sep='\t', names=[\"id\",\"text\",\"emotion\",\"intensity\"], engine='python')\n",
    "testDf=testDf.drop(\"id\",axis=1)\n",
    "print(\"Initial Shape of Test data:\",testDf.shape)\n",
    "\n",
    "\n",
    "AngerDf = testDf[testDf[\"emotion\"]==\"anger\"].drop(\"emotion\",axis=1)\n",
    "AngerDf = AngerDf[AngerDf[\"intensity\"]>=0.400]\n",
    "AngerDf[\"emotion\"] = 0\n",
    "\n",
    "FearDf = testDf[testDf[\"emotion\"]==\"fear\"].drop(\"emotion\",axis=1)\n",
    "FearDf = FearDf[FearDf[\"intensity\"]>=0.438]\n",
    "FearDf[\"emotion\"] = 1\n",
    "\n",
    "JoyDf = testDf[testDf[\"emotion\"]==\"joy\"].drop(\"emotion\",axis=1)\n",
    "JoyDf = JoyDf[JoyDf[\"intensity\"]>=0.400]\n",
    "JoyDf[\"emotion\"] = 2\n",
    "\n",
    "SadDf = testDf[testDf[\"emotion\"]==\"sadness\"].drop(\"emotion\",axis=1)\n",
    "SadDf = SadDf[SadDf[\"intensity\"]>=0.354]\n",
    "SadDf[\"emotion\"] = 3\n",
    "\n",
    "testDf = pd.concat([AngerDf,FearDf,JoyDf,SadDf],ignore_index=True)\n",
    "print(\"Final shape of Test data:\",testDf.shape)\n",
    "print(testDf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train= trainDf.drop([\"intensity\"],axis=1)\n",
    "data_cv   = crossValDf.drop([\"intensity\"],axis=1)\n",
    "data_test = testDf.drop([\"intensity\"],axis=1)\n",
    "\n",
    "# emotion=0 means anger\n",
    "# emotion=1 means fear\n",
    "# emotion=2 means joy\n",
    "# emotion=3 means sadness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  emotion\n",
      "0  [fuk, heck, move, fridg, knock, landlord, door...        0\n",
      "1  [indian, uber, driver, call, someon, n, word, ...        0\n",
      "2  [ask, parcel, deliv, pick, store, address, fum...        0\n",
      "3  [ef, whichev, butt, wipe, pull, fire, alarm, d...        0\n",
      "4  [join, btcare, put, phone, talk, rude, take, m...        0\n",
      "5                                      [blood, boil]        0\n",
      "6  [still, got, whole, season, wentworth, watch, ...        0\n",
      "7  [track, show, equip, deliv, servic, sudden, de...        0\n",
      "8               [legit, furious, peopl, fuck, idiot]        0\n",
      "9             [suppos, work, wtf, dude, thank, piss]        0\n",
      "\n",
      "                                                text  emotion\n",
      "0   fuk heck move fridg knock landlord door angri...        0\n",
      "1   indian uber driver call someon n word move ve...        0\n",
      "2   ask parcel deliv pick store not address fume ...        0\n",
      "3   ef whichev butt wipe pull fire alarm davi bc ...        0\n",
      "4   join btcare put phone talk rude take money ac...        0\n",
      "5                                         blood boil        0\n",
      "6   still got whole season wentworth watch stupid...        0\n",
      "7   track show equip deliv servic sudden delay al...        0\n",
      "8                     legit furious peopl fuck idiot        0\n",
      "9                     suppo work wtf dude thank piss        0\n",
      "\n",
      "                                                text  emotion\n",
      "0                   [pls, dont, insult, word, molna]        0\n",
      "1        [would, almost, took, offens, actual, snap]        0\n",
      "2  [rutger, game, abomin, affront, god, man, must...        0\n",
      "3                [lisa, ask, start, rage, call, heh]        0\n",
      "4  [sometim, get, mad, someth, minuscul, tri, rui...        0\n",
      "5  [sometim, get, mad, someth, minuscul, tri, rui...        0\n",
      "6  [eye, dilat, hate, world, right, rage, thousan...        0\n",
      "7  [one, chosen, clp, member, mp, seat, peopl, do...        0\n",
      "8  [one, chosen, clp, member, mp, seat, peopl, do...        0\n",
      "9  [pleas, canadian, player, play, us, player, la...        0\n",
      "\n",
      "                                                text  emotion\n",
      "0                         pls dont insult word molna        0\n",
      "1               would almost took offens actual snap        0\n",
      "2   rutger game abomin affront god man must never...        0\n",
      "3                       lisa ask start rage call heh        0\n",
      "4   sometim get mad someth minuscul tri ruin some...        0\n",
      "5   sometim get mad someth minuscul tri ruin some...        0\n",
      "6   eye dilat hate world right rage thousand fier...        0\n",
      "7   one chosen clp member mp seat peopl dole mate...        0\n",
      "8   one chosen clp member mp seat peopl dole mate...        0\n",
      "9   pleas canadian player play us player lag atro...        0\n",
      "\n",
      "                                                text  emotion\n",
      "0  [game, piss, game, year, blood, boil, time, tu...        0\n",
      "1  [mrsajhargreav, melli, garybarlow, come, mum, ...        0\n",
      "2  [mrsajhargreav, melli, garybarlow, come, mum, ...        0\n",
      "3  [disconnect, whilst, holiday, move, hous, st, ...        0\n",
      "4                    [ur, road, rage, give, anxieti]        0\n",
      "5                                          [furious]        0\n",
      "6  [gonna, give, kkk, polic, bitch, minimum, sent...        0\n",
      "7  [gonna, give, kkk, polic, bitch, minimum, sent...        0\n",
      "8                              [got, murder, madden]        0\n",
      "9  [new, madden, video, gonna, xbox, ahol, go, st...        0\n",
      "                                                text  emotion\n",
      "0   game piss game year blood boil time turn stlcard        0\n",
      "1   mrsajhargreav melli garybarlow come mum th k ...        0\n",
      "2   mrsajhargreav melli garybarlow come mum th k ...        0\n",
      "3   disconnect whilst holiday move hous st octob ...        0\n",
      "4                          ur road rage give anxieti        0\n",
      "5                                            furious        0\n",
      "6   gonna give kkk polic bitch minimum sentenc wa...        0\n",
      "7   gonna give kkk polic bitch minimum sentenc wa...        0\n",
      "8                                  got murder madden        0\n",
      "9        new madden video gonna xbox ahol go struggl        0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "#Get set of english stop words and prepare stemmer\n",
    "#Stop words for bag of words are different because we will use bigrams\n",
    "stop=set(stopwords.words(\"english\"))\n",
    "stop_bow = set(stopwords.words(\"english\"))\n",
    "stop_bow.discard(\"not\")\n",
    "stop_bow.discard(\"no\")\n",
    "sno = nltk.stem.SnowballStemmer(\"english\")\n",
    "\n",
    "#Train data cleaning\n",
    "train_text=data_train[\"text\"]\n",
    "cleaned_text_bow=[]\n",
    "cleaned_text=[]\n",
    "for line in train_text:\n",
    "    #Removing tags(ex-@abhishek is a name and not needed)\n",
    "    tags = re.compile(\"^@[a-zA-Z_]*\")\n",
    "    line = re.sub(tags,\" \",line)\n",
    "    #Replacing # and * with a \"\"\n",
    "    hashtags = re.compile(\"#|\\*\")\n",
    "    line = re.sub(hashtags,\"\",line)\n",
    "    #Replacing all other characters with a space\n",
    "    extraCharacters = re.compile(\"[^a-zA-Z]\")\n",
    "    line = re.sub(extraCharacters,\" \",line)\n",
    "\n",
    "    #Conversion of line to array of words for word2vec\n",
    "    filtered_words=[]\n",
    "    filtered_words_bow=\"\"\n",
    "    for word in line.split():\n",
    "        word=word.lower()\n",
    "        if(word not in stop):\n",
    "            word = sno.stem(word)\n",
    "            filtered_words.append(word)\n",
    "        if(word not in stop_bow):\n",
    "            word = sno.stem(word)\n",
    "            filtered_words_bow+=\" \"+word\n",
    "    cleaned_text.append(filtered_words)\n",
    "    cleaned_text_bow.append(filtered_words_bow)\n",
    "            \n",
    "data_train_bow = pd.DataFrame(data=cleaned_text_bow,columns=[\"text\"])\n",
    "data_train_bow[\"emotion\"] = data_train[\"emotion\"]\n",
    "data_train[\"text\"]=cleaned_text\n",
    "print(data_train.head(10))\n",
    "print()\n",
    "print(data_train_bow.head(10))\n",
    "print()\n",
    "\n",
    "#Cross Validate data cleaning\n",
    "cv_text=data_cv[\"text\"][0:]\n",
    "cleaned_text=[]\n",
    "cleaned_text_bow=[]\n",
    "for line in cv_text:\n",
    "    #Removing tags(ex-@abhishek is a name and not needed)\n",
    "    tags = re.compile(\"^@[a-zA-Z_]*\")\n",
    "    line = re.sub(tags,\" \",line)\n",
    "    #Replacing # and * with \"\" \n",
    "    hashtags = re.compile(\"#|\\*\")\n",
    "    line = re.sub(hashtags,\"\",line)\n",
    "    #Replacing all other characters with a space\n",
    "    extraCharacters = re.compile(\"[^a-zA-Z]\")\n",
    "    line = re.sub(extraCharacters,\" \",line)\n",
    "    \n",
    "    #Conversion of line to array of words\n",
    "    filtered_words=[]\n",
    "    filtered_words_bow=\"\"\n",
    "    for word in line.split():\n",
    "        word=word.lower()\n",
    "        if(word not in stop):\n",
    "            word = sno.stem(word)\n",
    "            filtered_words.append(word)\n",
    "            filtered_words_bow+=\" \"+word\n",
    "    cleaned_text.append(filtered_words)\n",
    "    cleaned_text_bow.append(filtered_words_bow)\n",
    "            \n",
    "data_cv_bow = pd.DataFrame(data=cleaned_text_bow, columns=[\"text\"])\n",
    "data_cv_bow[\"emotion\"]=data_cv[\"emotion\"]\n",
    "data_cv[\"text\"]=cleaned_text\n",
    "print(data_cv.head(10))\n",
    "print()\n",
    "print(data_cv_bow.head(10))\n",
    "print()\n",
    "\n",
    "#Test data cleaning\n",
    "test_text=data_test[\"text\"][0:]\n",
    "cleaned_text=[]\n",
    "cleaned_text_bow=[]\n",
    "for line in test_text:\n",
    "    #Removing tags(ex-@abhishek is a name and not needed)\n",
    "    tags = re.compile(\"^@[a-zA-Z_]*\")\n",
    "    line = re.sub(tags,\" \",line)\n",
    "    #Replacing hash and * with \"\"\n",
    "    hashtags = re.compile(\"#|\\*\")\n",
    "    line = re.sub(hashtags,\"\",line)\n",
    "    #Replacing all other characters with a space\n",
    "    extraCharacters = re.compile(\"[^a-zA-Z]\")\n",
    "    line = re.sub(extraCharacters,\" \",line)\n",
    "    \n",
    "    #Conversion of line to array of words\n",
    "    filtered_words=[]\n",
    "    filtered_words_bow=\"\"\n",
    "    for word in line.split():\n",
    "        word=word.lower()\n",
    "        if(word not in stop):\n",
    "            word = sno.stem(word)\n",
    "            filtered_words.append(word)\n",
    "            filtered_words_bow+=\" \"+word\n",
    "    cleaned_text.append(filtered_words)\n",
    "    cleaned_text_bow.append(filtered_words_bow)\n",
    "            \n",
    "data_test_bow = pd.DataFrame(data=cleaned_text_bow, columns=[\"text\"])\n",
    "data_test_bow[\"emotion\"]=data_test[\"emotion\"]\n",
    "data_test[\"text\"]=cleaned_text\n",
    "print(data_test.head(10))\n",
    "print(data_test_bow.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2840/2840 [00:00<00:00, 5020.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.010851 -0.003688 -0.001901 -0.011896  0.003534 -0.009506 -0.012363   \n",
      "1  0.016236 -0.005182 -0.004301 -0.019407  0.005657 -0.015649 -0.019823   \n",
      "2  0.008532 -0.003147 -0.001670 -0.010332  0.003441 -0.008199 -0.010699   \n",
      "3  0.009729 -0.003736 -0.002111 -0.011186  0.003618 -0.008811 -0.011629   \n",
      "4  0.014149 -0.005358 -0.003964 -0.016960  0.005082 -0.013828 -0.016283   \n",
      "5  0.008384 -0.003335 -0.002157 -0.011122  0.003437 -0.009354 -0.009939   \n",
      "6  0.020037 -0.007843 -0.004988 -0.024805  0.007998 -0.019729 -0.024252   \n",
      "7  0.011678 -0.004402 -0.002129 -0.014377  0.004995 -0.011226 -0.013769   \n",
      "8  0.021942 -0.008933 -0.005779 -0.027797  0.007854 -0.022617 -0.026581   \n",
      "9  0.011288 -0.004778 -0.002088 -0.013435  0.004394 -0.010876 -0.012735   \n",
      "\n",
      "          7         8         9  ...       291       292       293       294  \\\n",
      "0 -0.003398  0.006141 -0.006827  ... -0.006243 -0.003452 -0.003920 -0.019083   \n",
      "1 -0.005349  0.009646 -0.010024  ... -0.010741 -0.005815 -0.005584 -0.030214   \n",
      "2 -0.002756  0.005674 -0.004851  ... -0.005585 -0.003276 -0.003050 -0.017000   \n",
      "3 -0.003118  0.005661 -0.005361  ... -0.005780 -0.003353 -0.003140 -0.017822   \n",
      "4 -0.004743  0.008435 -0.008578  ... -0.009249 -0.005474 -0.004884 -0.026394   \n",
      "5 -0.003644  0.006260 -0.006119  ... -0.005865 -0.002844 -0.001974 -0.016137   \n",
      "6 -0.007050  0.011544 -0.012667  ... -0.012845 -0.007176 -0.007366 -0.038681   \n",
      "7 -0.004119  0.007598 -0.006900  ... -0.007147 -0.003716 -0.004781 -0.022620   \n",
      "8 -0.007577  0.013525 -0.013125  ... -0.013131 -0.007749 -0.008227 -0.043711   \n",
      "9 -0.003848  0.006318 -0.006600  ... -0.007256 -0.003579 -0.003682 -0.019847   \n",
      "\n",
      "        295       296       297       298       299  emotion  \n",
      "0  0.004366  0.013508  0.013858 -0.001787  0.001079        0  \n",
      "1  0.006430  0.021205  0.021667 -0.002984  0.000483        0  \n",
      "2  0.003360  0.010550  0.012139 -0.001286 -0.000245        0  \n",
      "3  0.003893  0.012718  0.012915 -0.001242  0.000439        0  \n",
      "4  0.005436  0.018104  0.019207 -0.002981  0.000488        0  \n",
      "5  0.003586  0.012184  0.011282 -0.002273 -0.000248        0  \n",
      "6  0.008887  0.026010  0.027885 -0.003805  0.000692        0  \n",
      "7  0.004238  0.014908  0.016092 -0.001962 -0.000124        0  \n",
      "8  0.009315  0.028110  0.029800 -0.005134  0.000471        0  \n",
      "9  0.004069  0.014342  0.014410 -0.001775 -0.000505        0  \n",
      "\n",
      "[10 rows x 301 columns]\n",
      "(2840, 301)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Train on text data\n",
    "list_of_sent = data_train[\"text\"]\n",
    "w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=300, workers=4)\n",
    "w2v_words=list(w2v_model.wv.vocab)\n",
    "\n",
    "#Vectorize text data\n",
    "listof_sent_vec=[]\n",
    "#tqdm is for improving speed and making progressbar\n",
    "#Vectorization and normalization both going on\n",
    "for sent in tqdm(list_of_sent): \n",
    "    sent_vec = np.zeros(300) \n",
    "    cnt_words =0; \n",
    "    for word in sent: \n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    listof_sent_vec.append(sent_vec)\n",
    "    \n",
    "Label = data_train[\"emotion\"]\n",
    "list_col=tuple(range(300))\n",
    "W2v_data_train=pd.DataFrame(data=listof_sent_vec, columns=list_col)\n",
    "W2v_data_train[\"emotion\"] = Label\n",
    "print(W2v_data_train.head(10))\n",
    "print(W2v_data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 296/296 [00:00<00:00, 5284.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.007539 -0.001739 -0.002184 -0.010123  0.002469 -0.007762 -0.010161   \n",
      "1  0.013271 -0.004445 -0.003466 -0.015164  0.004492 -0.012766 -0.015528   \n",
      "2  0.016365 -0.005355 -0.003755 -0.020174  0.005800 -0.015246 -0.018939   \n",
      "3  0.021701 -0.007591 -0.005601 -0.025561  0.007609 -0.021019 -0.025437   \n",
      "4  0.023080 -0.008594 -0.005644 -0.028017  0.008958 -0.021951 -0.027621   \n",
      "5  0.023248 -0.008580 -0.005676 -0.028127  0.008905 -0.022185 -0.027888   \n",
      "6  0.016402 -0.006142 -0.004028 -0.019758  0.005558 -0.016065 -0.019404   \n",
      "7  0.013988 -0.004883 -0.002979 -0.016873  0.005327 -0.014061 -0.017099   \n",
      "8  0.014440 -0.005219 -0.003109 -0.017067  0.005494 -0.014584 -0.017756   \n",
      "9  0.016108 -0.005761 -0.003019 -0.018608  0.005366 -0.016218 -0.019776   \n",
      "\n",
      "          7         8         9  ...       291       292       293       294  \\\n",
      "0 -0.003265  0.004782 -0.005060  ... -0.006096 -0.002846 -0.002391 -0.016000   \n",
      "1 -0.004029  0.008153 -0.007530  ... -0.009364 -0.004669 -0.004100 -0.023769   \n",
      "2 -0.005145  0.009560 -0.009567  ... -0.010396 -0.005995 -0.005933 -0.030580   \n",
      "3 -0.006655  0.013007 -0.012787  ... -0.013666 -0.007895 -0.007876 -0.039730   \n",
      "4 -0.007761  0.013485 -0.014262  ... -0.014727 -0.008079 -0.008187 -0.043681   \n",
      "5 -0.007885  0.013462 -0.014320  ... -0.014890 -0.008092 -0.008223 -0.043799   \n",
      "6 -0.005415  0.009782 -0.010101  ... -0.010544 -0.005422 -0.006311 -0.031256   \n",
      "7 -0.004472  0.008343 -0.008855  ... -0.009420 -0.005182 -0.004845 -0.027714   \n",
      "8 -0.004671  0.008362 -0.009290  ... -0.009642 -0.005148 -0.004929 -0.028344   \n",
      "9 -0.005479  0.008819 -0.010185  ... -0.009437 -0.005925 -0.005448 -0.029652   \n",
      "\n",
      "        295       296       297       298       299  emotion  \n",
      "0  0.002907  0.010469  0.010266 -0.001623  0.000335        0  \n",
      "1  0.006027  0.015664  0.017387 -0.002496  0.000520        0  \n",
      "2  0.006581  0.020966  0.022522 -0.003714  0.000628        0  \n",
      "3  0.008812  0.027197  0.028870 -0.004591  0.000941        0  \n",
      "4  0.009225  0.029822  0.030979 -0.004396  0.000889        0  \n",
      "5  0.009354  0.029915  0.031081 -0.004387  0.000894        0  \n",
      "6  0.007013  0.020533  0.022384 -0.002883  0.000503        0  \n",
      "7  0.005646  0.017938  0.019446 -0.002767 -0.000348        0  \n",
      "8  0.005870  0.018555  0.019817 -0.002980 -0.000295        0  \n",
      "9  0.006707  0.020407  0.021507 -0.003244  0.000916        0  \n",
      "\n",
      "[10 rows x 301 columns]\n",
      "(296, 301)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 998/998 [00:00<00:00, 5431.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.016952 -0.005859 -0.003993 -0.020428  0.005976 -0.016370 -0.019936   \n",
      "1  0.010692 -0.003514 -0.001827 -0.013096  0.005031 -0.010432 -0.013138   \n",
      "2  0.011080 -0.003557 -0.002226 -0.013910  0.005364 -0.010874 -0.014093   \n",
      "3  0.008669 -0.003210 -0.002274 -0.011089  0.002638 -0.008562 -0.010944   \n",
      "4  0.018090 -0.006539 -0.005328 -0.022068  0.006712 -0.017034 -0.022339   \n",
      "5  0.005584 -0.003517 -0.000309 -0.007823  0.002723 -0.006999 -0.007717   \n",
      "6  0.015865 -0.006010 -0.004033 -0.019874  0.007076 -0.014708 -0.020321   \n",
      "7  0.014795 -0.005594 -0.004093 -0.019207  0.006835 -0.014104 -0.019387   \n",
      "8  0.016857 -0.005269 -0.004303 -0.020720  0.006373 -0.015609 -0.018732   \n",
      "9  0.016815 -0.006123 -0.003878 -0.021644  0.007037 -0.017383 -0.020802   \n",
      "\n",
      "          7         8         9  ...       291       292       293       294  \\\n",
      "0 -0.005693  0.010089 -0.010066  ... -0.011036 -0.006037 -0.005523 -0.031348   \n",
      "1 -0.003879  0.006598 -0.006791  ... -0.006453 -0.003416 -0.004076 -0.020991   \n",
      "2 -0.003813  0.007177 -0.007141  ... -0.006840 -0.003501 -0.004220 -0.022426   \n",
      "3 -0.002672  0.005062 -0.005150  ... -0.005284 -0.003081 -0.003615 -0.015876   \n",
      "4 -0.005599  0.011125 -0.011011  ... -0.011049 -0.006057 -0.007191 -0.034482   \n",
      "5 -0.001326  0.003688 -0.002131  ... -0.002205 -0.000887 -0.002295 -0.011759   \n",
      "6 -0.004385  0.009286 -0.009187  ... -0.008912 -0.005734 -0.006011 -0.030458   \n",
      "7 -0.004292  0.008629 -0.008408  ... -0.008464 -0.005367 -0.005731 -0.029458   \n",
      "8 -0.004603  0.009092 -0.010268  ... -0.009984 -0.005462 -0.007085 -0.030426   \n",
      "9 -0.005094  0.009267 -0.010351  ... -0.010533 -0.007395 -0.006864 -0.032657   \n",
      "\n",
      "        295       296       297       298       299  emotion  \n",
      "0  0.006977  0.022132  0.022190 -0.004073  0.001118        0  \n",
      "1  0.004032  0.013682  0.014673 -0.001889  0.001107        0  \n",
      "2  0.004575  0.014275  0.015663 -0.002308  0.001103        0  \n",
      "3  0.004094  0.010608  0.011635 -0.001228 -0.000113        0  \n",
      "4  0.008129  0.023108  0.023926 -0.003635  0.000461        0  \n",
      "5  0.003396  0.006315  0.006506 -0.000873 -0.001011        0  \n",
      "6  0.006577  0.019713  0.021176 -0.002857  0.000425        0  \n",
      "7  0.006521  0.018792  0.020564 -0.002933  0.000188        0  \n",
      "8  0.007511  0.020764  0.021136 -0.002966 -0.000084        0  \n",
      "9  0.007620  0.022303  0.022887 -0.003044  0.000328        0  \n",
      "\n",
      "[10 rows x 301 columns]\n",
      "(998, 301)\n"
     ]
    }
   ],
   "source": [
    "#Vectorize Cross Validate\n",
    "list_of_sent= data_cv[\"text\"]\n",
    "listof_sent_vec=[]\n",
    "\n",
    "for sent in tqdm(list_of_sent): \n",
    "    sent_vec = np.zeros(300) \n",
    "    cnt_words =0; \n",
    "    for word in sent: \n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    listof_sent_vec.append(sent_vec)\n",
    "    \n",
    "Label = data_cv[\"emotion\"]\n",
    "list_col=tuple(range(300))\n",
    "W2v_data_cv=pd.DataFrame(data=listof_sent_vec, columns=list_col)\n",
    "W2v_data_cv[\"emotion\"] = Label\n",
    "print(W2v_data_cv.head(10))\n",
    "print(W2v_data_cv.shape)\n",
    "\n",
    "\n",
    "#test\n",
    "list_of_sent= data_test[\"text\"]\n",
    "listof_sent_vec=[]\n",
    "\n",
    "for sent in tqdm(list_of_sent): \n",
    "    sent_vec = np.zeros(300) \n",
    "    cnt_words =0; \n",
    "    for word in sent: \n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    listof_sent_vec.append(sent_vec)\n",
    "    \n",
    "Label = data_test[\"emotion\"]\n",
    "list_col=tuple(range(300))\n",
    "W2v_data_test=pd.DataFrame(data=listof_sent_vec, columns=list_col)\n",
    "W2v_data_test[\"emotion\"] = Label\n",
    "print(W2v_data_test.head(10))\n",
    "print(W2v_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape of Train X and y for word2vec: (2840, 300) (2840,)\n",
      "Final shape of CV X and y for word2vec: (296, 300) (296,)\n",
      "Final shape of Test X and y for word2vec: (998, 300) (998,)\n"
     ]
    }
   ],
   "source": [
    "X_train_w2v = W2v_data_train.drop(\"emotion\",axis=1).to_numpy()\n",
    "y_train_w2v = W2v_data_train[\"emotion\"].to_numpy()\n",
    "\n",
    "X_cv_w2v = W2v_data_cv.drop(\"emotion\",axis=1).to_numpy()\n",
    "y_cv_w2v = W2v_data_cv[\"emotion\"].to_numpy()\n",
    "\n",
    "X_test_w2v = W2v_data_test.drop(\"emotion\",axis=1).to_numpy()\n",
    "y_test_w2v = W2v_data_test[\"emotion\"].to_numpy()\n",
    "\n",
    "print(\"Final shape of Train X and y for word2vec:\",X_train.shape,y_train.shape)\n",
    "print(\"Final shape of CV X and y for word2vec:\",X_cv.shape,y_cv.shape)\n",
    "print(\"Final shape of Test X and y for word2vec:\",X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization(bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape of Train X and y for bag of words: (2840, 24681) (2840,)\n",
      "Final shape of CV X and y for bag of words: (296, 24681) (296,)\n",
      "Final shape of Test X and y for bag of words: (998, 24681) (998,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#Train bag of words\n",
    "bow_model = CountVectorizer(ngram_range=(1,2))\n",
    "bow_model.fit(data_train_bow[\"text\"])\n",
    "\n",
    "X_train_bow = normalize(bow_model.transform(data_train_bow[\"text\"])).tocsr()\n",
    "y_train_bow = data_train_bow[\"emotion\"].to_numpy()\n",
    "\n",
    "X_cv_bow = normalize(bow_model.transform(data_cv_bow[\"text\"])).tocsr()\n",
    "y_cv_bow = data_cv_bow[\"emotion\"].to_numpy()\n",
    "\n",
    "X_test_bow = normalize(bow_model.transform(data_test_bow[\"text\"])).tocsr()\n",
    "y_test_bow = data_test_bow[\"emotion\"].to_numpy()\n",
    "\n",
    "print(\"Final shape of Train X and y for bag of words:\",X_train_bow.shape,y_train_bow.shape)\n",
    "print(\"Final shape of CV X and y for bag of words:\",X_cv_bow.shape,y_cv_bow.shape)\n",
    "print(\"Final shape of Test X and y for bag of words:\",X_test_bow.shape,y_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
